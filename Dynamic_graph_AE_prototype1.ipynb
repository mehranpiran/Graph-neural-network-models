{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8910f-afcf-47e5-9f31-69274e77cece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, MessagePassing\n",
    "from torch_geometric_temporal.nn.recurrent import GConvLSTM\n",
    "from torch_geometric.utils import softmax\n",
    "import math\n",
    "\n",
    "class DynamicHGTConv(MessagePassing):\n",
    "    def __init__(self, in_dim, out_dim, num_types, num_relations, n_heads, dropout=0.2, use_norm=True, use_RTE=True, **kwargs):\n",
    "        super(DynamicHGTConv, self).__init__(node_dim=0, aggr='add', **kwargs)\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_types = num_types\n",
    "        self.num_relations = num_relations\n",
    "        self.total_rel = num_types * num_relations * num_types\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = out_dim // n_heads\n",
    "        self.sqrt_dk = math.sqrt(self.d_k)\n",
    "        self.use_norm = use_norm\n",
    "        self.use_RTE = use_RTE\n",
    "        self.att = None\n",
    "        self.res_att = None\n",
    "        self.res = None\n",
    "\n",
    "        self.k_linears = nn.ModuleList()\n",
    "        self.q_linears = nn.ModuleList()\n",
    "        self.v_linears = nn.ModuleList()\n",
    "        self.a_linears = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "\n",
    "        for t in range(num_types):\n",
    "            self.k_linears.append(nn.Linear(in_dim, out_dim))\n",
    "            self.q_linears.append(nn.Linear(in_dim, out_dim))\n",
    "            self.v_linears.append(nn.Linear(in_dim, out_dim))\n",
    "            self.a_linears.append(nn.Linear(out_dim, out_dim))\n",
    "            if use_norm:\n",
    "                self.norms.append(nn.LayerNorm(out_dim))\n",
    "\n",
    "        self.relation_pri = nn.Parameter(torch.ones(num_relations, self.n_heads))\n",
    "        self.relation_att = nn.Parameter(torch.Tensor(num_relations, n_heads, self.d_k, self.d_k))\n",
    "        self.relation_msg = nn.Parameter(torch.Tensor(num_relations, n_heads, self.d_k, self.d_k))\n",
    "        self.skip = nn.Parameter(torch.ones(num_types))\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        if self.use_RTE:\n",
    "            self.emb = RelTemporalEncoding(in_dim)\n",
    "\n",
    "        glorot(self.relation_att)\n",
    "        glorot(self.relation_msg)\n",
    "\n",
    "    def forward(self, node_inp, node_type, edge_index, edge_type, edge_time):\n",
    "        return self.propagate(edge_index, node_inp=node_inp, node_type=node_type, edge_type=edge_type, edge_time=edge_time)\n",
    "\n",
    "    def message(self, edge_index_i, node_inp_i, node_inp_j, node_type_i, node_type_j, edge_type, edge_time):\n",
    "        data_size = edge_index_i.size(0)\n",
    "        self.res_att = torch.zeros(data_size, self.n_heads).to(node_inp_i.device)\n",
    "        res_msg = torch.zeros(data_size, self.n_heads, self.d_k).to(node_inp_i.device)\n",
    "\n",
    "        for source_type in range(self.num_types):\n",
    "            sb = (node_type_j == int(source_type))\n",
    "            k_linear = self.k_linears[source_type]\n",
    "            v_linear = self.v_linears[source_type]\n",
    "            for target_type in range(self.num_types):\n",
    "                tb = (node_type_i == int(target_type)) & sb\n",
    "                q_linear = self.q_linears[target_type]\n",
    "                for relation_type in range(self.num_relations):\n",
    "                    idx = (edge_type == int(relation_type)) & tb\n",
    "                    if idx.sum() == 0:\n",
    "                        continue\n",
    "                    target_node_vec = node_inp_i[idx]\n",
    "                    source_node_vec = node_inp_j[idx]\n",
    "                    if self.use_RTE:\n",
    "                        source_node_vec = self.emb(source_node_vec, edge_time[idx])\n",
    "\n",
    "                    q_mat = q_linear(target_node_vec).view(-1, self.n_heads, self.d_k)\n",
    "                    k_mat = k_linear(source_node_vec).view(-1, self.n_heads, self.d_k)\n",
    "                    k_mat = torch.bmm(k_mat.transpose(1, 0), self.relation_att[relation_type]).transpose(1, 0)\n",
    "                    self.res_att[idx] = (q_mat * k_mat).sum(dim=-1) * self.relation_pri[relation_type] / self.sqrt_dk\n",
    "\n",
    "                    v_mat = v_linear(source_node_vec).view(-1, self.n_heads, self.d_k)\n",
    "                    res_msg[idx] = torch.bmm(v_mat.transpose(1, 0), self.relation_msg[relation_type]).transpose(1, 0)\n",
    "\n",
    "        res = res_msg * softmax(self.res_att.view(-1, self.n_heads, 1), edge_index_i)\n",
    "        return res.view(-1, self.out_dim)\n",
    "\n",
    "    def update(self, aggr_out, node_inp, node_type):\n",
    "        aggr_out = F.gelu(aggr_out)\n",
    "        res = torch.zeros(aggr_out.size(0), self.out_dim).to(node_inp.device)\n",
    "        for target_type in range(self.num_types):\n",
    "            idx = (node_type == int(target_type))\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            trans_out = self.drop(self.a_linears[target_type](aggr_out[idx]))\n",
    "            alpha = torch.sigmoid(self.skip[target_type])\n",
    "            if self.use_norm:\n",
    "                res[idx] = self.norms[target_type](trans_out * alpha + node_inp[idx] * (1 - alpha))\n",
    "            else:\n",
    "                res[idx] = trans_out * alpha + node_inp[idx] * (1 - alpha)\n",
    "        self.res = res\n",
    "        return res\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(in_dim={}, out_dim={}, num_types={}, num_relations={})'.format(\n",
    "            self.__class__.__name__, self.in_dim, self.out_dim, self.num_types, self.num_relations)\n",
    "\n",
    "\n",
    "class DynamicGeneralConv(nn.Module):\n",
    "    def __init__(self, conv_name, in_hid, out_hid, num_types, num_relations, n_heads, dropout, use_norm=True, use_RTE=True):\n",
    "        super(DynamicGeneralConv, self).__init__()\n",
    "        self.conv_name = conv_name\n",
    "        self.res_att = None\n",
    "        self.res = None\n",
    "        if self.conv_name == 'hgt':\n",
    "            self.base_conv = DynamicHGTConv(in_hid, out_hid, num_types, num_relations, n_heads, dropout, use_norm, use_RTE)\n",
    "        elif self.conv_name == 'dense_hgt':\n",
    "            self.base_conv = DenseHGTConv(in_hid, out_hid, num_types, num_relations, n_heads, dropout, use_norm, use_RTE)\n",
    "        elif self.conv_name == 'gcn':\n",
    "            self.base_conv = GCNConv(in_hid, out_hid)\n",
    "        elif self.conv_name == 'gat':\n",
    "            self.base_conv = GATConv(in_hid, out_hid // n_heads, heads=n_heads)\n",
    "\n",
    "    def forward(self, meta_xs, node_type, edge_index, edge_type, edge_time):\n",
    "        if self.conv_name == 'hgt':\n",
    "            a = self.base_conv(meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "            self.res_att = self.base_conv.res_att\n",
    "            self.res = self.base_conv.res\n",
    "            return a\n",
    "        elif self.conv_name == 'gcn':\n",
    "            return self.base_conv(meta_xs, edge_index)\n",
    "        elif self.conv_name == 'gat':\n",
    "            return self.base_conv(meta_xs, edge_index)\n",
    "        elif self.conv_name == 'dense_hgt':\n",
    "            return self.base_conv(meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "\n",
    "\n",
    "class RelTemporalEncoding(nn.Module):\n",
    "    def __init__(self, n_hid, max_len=240, dropout=0.2):\n",
    "        super(RelTemporalEncoding, self).__init__()\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, n_hid, 2) * -(math.log(10000.0) / n_hid))\n",
    "        emb = nn.Embedding(max_len, n_hid)\n",
    "        emb.weight.data[:, 0::2] = torch.sin(position * div_term) / math.sqrt(n_hid)\n",
    "        emb.weight.data[:, 1::2] = torch.cos(position * div_term) / math.sqrt(n_hid)\n",
    "        emb.requires_grad = False\n",
    "        self.emb = emb\n",
    "        self.lin = nn.Linear(n_hid, n_hid)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return x + self.lin(self.emb(t))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
